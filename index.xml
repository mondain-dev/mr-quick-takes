<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title><![CDATA[MR Quick Takes]]></title><description><![CDATA[MR Quick Takes]]></description><link>https://github.com/mondain-dev/mr-quick-takes</link><generator>RSS for Node</generator><lastBuildDate>Fri, 04 Oct 2024 10:09:45 GMT</lastBuildDate><atom:link href="https://mondain-dev.github.io/mr-quick-takes/index.xml" rel="self" type="application/rss+xml"/><item><title><![CDATA[Comment on Model this by Sure]]></title><description><![CDATA[<p>So if understand this correctly, they used pre-existing tests to adapt vignettes and then had three arms of testing.<br />
1. Physicians with brains<br />
2. Physicians with brains and GPT<br />
3. GPT. </p>
<p>It is not that surprising that GPT does well on "vignettes adapted from established diagnostic reasoning examinations" - the answers, in some form or another, are almost certainly wholly contained within the training data. Perhaps the adaptation is novel enough that does not matter, but my bet is that the adaptations were exceedingly limited because good adaptations are bloody hard (or were already done on previous iterations of the test which would also be in training data). </p>
<p>The other thing to consider is that real life introduces biases and weightings based on your actual patient population. It is well known that the tests are done up with a couple of goals in mind:<br />
1. They are far more tuned to find incompetent physicians than to stratify within physicians.<br />
2. The vignettes reflect what is in the literature which may be outdated, may not be applicable for your practice area (e.g. Denver has different baseline numbers for a bunch of pulm stuff than sea level norms), and is not constrained by questions of patient compliance, payment issues, or social factors unless those are explicitly the focus of the question.<br />
3. The vignettes are designed to be scorable in a standardized fashion with limited areas for lawyering.<br />
4. A lot of these vignettes are keyed to how much the physician has memorized and definitely that is what residents study for boards have to achieve. When you have folks used to relying on memory they can become quite good it ... but will hit a performance wall once you start pushing the edge of human retention capabilities. </p>
<p>As such, this shows us that GPT is as good or better at finding the answers that physicians reason and recall their way to, but that in most all instances this provides GPT with an edge. And a lot of the heuristics that physicians adopt to either pass their boards (where the material is biased based on memory games) or to practice in real life (which has highly idiosyncratic weightings not reflective of the tests) are not great for the tests.</p>
<p>Is GPT just finding the answers in the training data? I dunno. Is it finding which permutation on a known question is most likely and matching? possibly. But these tests have been calibrated to measure human skill given the limitations of memory and to measure it for a certain purpose. </p>
<p>I mean I have used GPT in real life and so far it simply has not been good enough to replace my lower skilled providers at tasks like interviewing patients. </p>
<p>End of the day I see two possibilities:<br />
1. GPT is reasoning, for reals in the same way we do, and on some structured tasks is better than us.<br />
2. GPT is showing the limitations of tests calibrated for human performance curves and hacking an approximation via other means. </p>
<p>Given that I have been critical of these tests as wildly inadequate for physicians for years, I am biased towards the second. I would dearly love to know what some LLM can manage if we just train it off the reference books rather than scraping the entire internet where the test question and answers inevitably reside. </p>
<p>So any event, my model is, quite tentatively, that GPT will show us some deficiencies in testing and physicians will need to completely restructure training to deemphasize memorization, unaided recall, and the rest. We have needed to do this since the 90s, but memorization is likely the single biggest dividing line between physicians and failures that it gets tied into identity. </p>
<p>I also suspect that the physicians who find a robust way to integrate LLMs to focus more on the core reasoning, rather than recall of idiosyncrasies of vignette testing, will become much more productive with LLMs even if they end up not being capable of general reasoning. Context queued retrieval is extremely hard to train and if computers can dial that up to 10,000 for everyone that is huge even absent any other capabilities.</p>
<hr/><p>Comment URL: <a href="https://marginalrevolution.com/marginalrevolution/2024/10/model-this-7.html?commentID=160818773">https://marginalrevolution.com/marginalrevolution/2024/10/model-this-7.html?commentID=160818773</a></p><p>Post URL: <a href="https://marginalrevolution.com/marginalrevolution/2024/10/model-this-7.html?utm_source=rss&utm_medium=rss&utm_campaign=model-this-7">https://marginalrevolution.com/marginalrevolution/2024/10/model-this-7.html?utm_source=rss&utm_medium=rss&utm_campaign=model-this-7</a></p><p>Votes: 46⬆ 2⬇</p>]]></description><link>https://marginalrevolution.com/marginalrevolution/2024/10/model-this-7.html?commentID=160818773</link><guid isPermaLink="true">https://marginalrevolution.com/marginalrevolution/2024/10/model-this-7.html?commentID=160818773</guid><dc:creator><![CDATA[Sure]]></dc:creator><pubDate>Wed, 02 Oct 2024 18:27:05 GMT</pubDate></item><item><title><![CDATA[Comment on Model this by Gene]]></title><description><![CDATA[<p>Between the fact the diagnosis accuracy was actually 66% vs 62% (see other comment and tweet) and the other point about these being "cases", i.e. written up (by whom?) for consumption and therefore already 80% of the way there in terms of what people expect from actual medicine (i.e. an interpretation of a manifest condition)... I think the real story here is still the massive flying-car hype drowning out the signal.</p>
<hr/><p>Comment URL: <a href="https://marginalrevolution.com/marginalrevolution/2024/10/model-this-7.html?commentID=160818770">https://marginalrevolution.com/marginalrevolution/2024/10/model-this-7.html?commentID=160818770</a></p><p>Post URL: <a href="https://marginalrevolution.com/marginalrevolution/2024/10/model-this-7.html?utm_source=rss&utm_medium=rss&utm_campaign=model-this-7">https://marginalrevolution.com/marginalrevolution/2024/10/model-this-7.html?utm_source=rss&utm_medium=rss&utm_campaign=model-this-7</a></p><p>Votes: 68⬆ 4⬇</p>]]></description><link>https://marginalrevolution.com/marginalrevolution/2024/10/model-this-7.html?commentID=160818770</link><guid isPermaLink="true">https://marginalrevolution.com/marginalrevolution/2024/10/model-this-7.html?commentID=160818770</guid><dc:creator><![CDATA[Gene]]></dc:creator><pubDate>Wed, 02 Oct 2024 18:23:34 GMT</pubDate></item></channel></rss>